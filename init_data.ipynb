{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used for ploting csv data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import pandas as pd\n",
    "import lib.base_strategy as bs\n",
    "import lib.init_data_helper as idh\n",
    "from fractions import Fraction as frac\n",
    "from tests.test_all_tests import get_test_data_path\n",
    "\n",
    "from binance import AsyncClient, Client\n",
    "import lib.api_data as api # for Binance data\n",
    "from lib.get_coinbase_data import get_coinbase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv's\n",
    "\n",
    "# Collect these files from Kaggle here:\n",
    "# https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-ethereum\n",
    "list_of_csv = [\n",
    "    # TODO: figure out problems with 2017 data\n",
    "    # 'full_data__6__2017.csv', # don't use 2017 data, its not great\n",
    "    'full_data__6__2018.csv',\n",
    "    'full_data__6__2019.csv',\n",
    "    'full_data__6__2020.csv',\n",
    "    'full_data__6__2021.csv'\n",
    "]\n",
    "sorted_list_of_csv = [\n",
    "    # 'sorted_full_data_2017.csv',\n",
    "    'sorted_full_data_2018.csv',\n",
    "    'sorted_full_data_2019.csv',\n",
    "    'sorted_full_data_2020.csv',\n",
    "    'sorted_full_data_2021.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subset of data:\n",
      "   index   timestamp                  fraction_price  decimal_price\n",
      "0      0  1514764800  6508515100154921/8796093022208       739.9325\n",
      "1      1  1514764860                          1479/2       739.5000\n",
      "2      2  1514764920  1625809361085399/2199023255552       739.3325\n",
      "3      3  1514764980   812789231821783/1099511627776       739.2275\n",
      "4      4  1514765040  6505480448062259/8796093022208       739.5875\n",
      "\n",
      "Subset of data:\n",
      "   index   timestamp                  fraction_price  decimal_price\n",
      "0      0  1514768400  6461499982951219/8796093022208       734.5875\n",
      "1      1  1514768460  6461521973183775/8796093022208       734.5900\n",
      "2      2  1514768520   807684749089833/1099511627776       734.5850\n",
      "3      3  1514768580  6461324061090775/8796093022208       734.5675\n",
      "4      4  1514768640  6461499982951219/8796093022208       734.5875\n",
      "    index   timestamp                    fraction_price  decimal_price\n",
      "0       0  1514764800    3242503770776535/4398046511104       737.2600\n",
      "1       1  1514764860  12966232763106591/17592186044416       737.0450\n",
      "2       2  1514764920    3241178859265065/4398046511104       736.9588\n",
      "3       3  1514764980  12963637915665039/17592186044416       736.8975\n",
      "4       4  1514765040    6483490215506739/8796093022208       737.0875\n",
      "..    ...         ...                               ...            ...\n",
      "56     56  1514768160    3220376099267543/4398046511104       732.2287\n",
      "57     57  1514768220    6439432784581755/8796093022208       732.0788\n",
      "58     58  1514768280  12882757840325837/17592186044416       732.3000\n",
      "59     59  1514768340  12882779830558393/17592186044416       732.3012\n",
      "60     60  1514768400  12884736961255833/17592186044416       732.4125\n",
      "\n",
      "[61 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get CoinBase Data\n",
    "# \"\"\"\n",
    "# HistoricalData() info:\n",
    "# | ticker | supply the ticker information which you want to return (str). | | granularity | please supply a granularity in seconds (60, 300, 900, 3600, 21600, 86400) (int). | | start_date | a string in the format YYYY-MM-DD-HH-MM (str). | | end_date | a string in the format YYYY-MM-DD-HH-MM (str). Optional, Default: Now | | verbose | printing during extraction. Default: True |\n",
    "# \"\"\"\n",
    "# def get_coinbase_data(start_date, end_date=''):\n",
    "#     \"\"\"Get data from CoinBase Pro API\"\"\"\n",
    "#     # how many seconds between data points\n",
    "#     trade_interval = 60\n",
    "#     if end_date == '':\n",
    "#         # Returns data as a dataframe\n",
    "#         coinbase_data = HistoricalData(\n",
    "#             'ETH-USD',\n",
    "#             trade_interval,\n",
    "#             start_date=start_date,\n",
    "#             # end_date='2019-01-6-00-00', # Comment out to use current time as end\n",
    "#             verbose=False\n",
    "#         ).retrieve_data()\n",
    "#     else:\n",
    "#         # Returns data as a dataframe\n",
    "#         coinbase_data = HistoricalData(\n",
    "#             'ETH-USD',\n",
    "#             trade_interval,\n",
    "#             start_date=start_date,\n",
    "#             end_date=end_date, # Comment out to use current time as end\n",
    "#             verbose=False\n",
    "#         ).retrieve_data()\n",
    "\n",
    "#     # make time no longer the index and rename it\n",
    "#     coinbase_data.reset_index(inplace=True)\n",
    "#     coinbase_data.rename(columns = {'time':'timestamp'}, inplace = True)\n",
    "#     # Make 'index' a regular column\n",
    "#     coinbase_data['index'] = coinbase_data.index\n",
    "\n",
    "#     # turn timestamp from a datetime into a timestamp, divide by 10**9 to get seconds as our units\n",
    "#     coinbase_data['timestamp'] = coinbase_data['timestamp'].apply(lambda x: int(x.value// 10**9))\n",
    "#     # Create price data\n",
    "#     coinbase_data['decimal_price'] = round(coinbase_data['open'] + coinbase_data['close'] + coinbase_data['high'] + coinbase_data['low'], 4)\n",
    "#     # Then fractionalize the number to minimize floating point rounding errors\n",
    "#     coinbase_data['fraction_price'] = coinbase_data['decimal_price'].apply(lambda x: frac(x)/4)\n",
    "#     # Average out decimal price\n",
    "#     coinbase_data['decimal_price'] = coinbase_data['decimal_price']/4\n",
    "#     # drop all columns we don't want\n",
    "#     coinbase_data = coinbase_data.filter(['index', 'timestamp', 'fraction_price', 'decimal_price'])\n",
    "\n",
    "#     print(f'\\nSubset of data:\\n{coinbase_data.head(5)}')\n",
    "#     return coinbase_data\n",
    "\n",
    "# Split data collection into two parts so that it doesn't error out\n",
    "part_1 = get_coinbase_data(start_date='2018-01-01-00-00', end_date='2018-01-1-01-00')\n",
    "part_2 = get_coinbase_data(start_date='2018-01-01-01-00', end_date='2018-01-1-02-00')\n",
    "coinbase_data = idh.combine_datasets(part_1, part_2)\n",
    "print(coinbase_data)\n",
    "\n",
    "# part_1 = get_coinbase_data(start_date='2018-01-01-00-00', end_date='2020-01-1-00-00')\n",
    "# # Don't pass end_date to use current time as end\n",
    "# part_2 = get_coinbase_data(start_date='2020-01-01-00-00')\n",
    "# # Combine parts\n",
    "# coinbase_data = idh.combine_datasets(part_1, part_2)\n",
    "# # Give index a name\n",
    "# coinbase_data.index.names = ['index']\n",
    "# # TODO: test that index gets a name in the resulting csv RENAME ENDING FILE!!!!\n",
    "# # Save data\n",
    "# print(f'\\nHistorical values found: {len(coinbase_data.index)}')\n",
    "# coinbase_data.to_csv(bs.full_path('CoinBase_ETH_all_price_data'))\n",
    "# print(f'\\nSubset of final data:\\n{coinbase_data.tail(5)}')\n",
    "# print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Binance Data\n",
    "\n",
    "# Initialise the client\n",
    "# NOTE: You will have to supply your OWN api keys to use this\n",
    "client = Client(api.get_api_key(), api.get_secret())\n",
    "# USDT has more historical price data than USDC\n",
    "trading_pair = 'ETHUSDT'\n",
    "# trading_pair = 'ETHUSDC'\n",
    "\n",
    "if trading_pair == 'ETHUSDT':\n",
    "    ## For ETH USDT\n",
    "    # Earliest timestamp found: 1502942400.0\n",
    "    # Human readable format: Wed Aug 16 21:00:00 2017\n",
    "    # However, early Binance ETH USDT data is not very good so only use 2018+ (aka 1514764800+)\n",
    "    # Multiply by 1000 to make what we would get from the query\n",
    "    timestamp = 1514764800*1000\n",
    "else:\n",
    "    # get timestamp of earliest date data is available\n",
    "    timestamp = client._get_earliest_valid_timestamp(trading_pair, AsyncClient.KLINE_INTERVAL_1MINUTE)\n",
    "    print(f'Earliest timestamp found: {int(timestamp)/1000}')\n",
    "    print(f'Human readable format: {time.ctime(int(timestamp)/1000)}')\n",
    "    ## For ETH USDC (not quite as much data as USDT)\n",
    "    # Earliest timestamp found: 1544844060.0\n",
    "    # Human readable format: Fri Dec 14 19:21:00 2018\n",
    "\n",
    "# query_start = '1 Jan, 2019'\n",
    "# query_end = '6 Jan, 2019'\n",
    "# # Get data between two specific times\n",
    "# klines = client.get_historical_klines(trading_pair, AsyncClient.KLINE_INTERVAL_1MINUTE, start_str=query_start, end_str=query_end)\n",
    "\n",
    "# Get data from the beginning of the Binance data to now\n",
    "klines = client.get_historical_klines(trading_pair, AsyncClient.KLINE_INTERVAL_1MINUTE, timestamp)\n",
    "\n",
    "print(f'Historical values found: {len(klines)}')\n",
    "print(f'Subset of data:\\n{klines[0:5]}')\n",
    "\n",
    "# save data as a file\n",
    "save_file_name = 'csv_files/Binance_ETH_all_price_data.csv'\n",
    "with open(save_file_name, 'w') as d:\n",
    "# with open('csv_files/ETH_all_price_data.csv', 'w') as d:\n",
    "    d.write('index,timestamp,fraction_price,decimal_price\\n')\n",
    "    # add index column\n",
    "    ind = 0\n",
    "    for line in klines:\n",
    "        fraction_price = frac(float(line[1])+float(line[2])+float(line[3])+float(line[4]))/4\n",
    "        decimal_price = round(fraction_price, 4)\n",
    "        # Divide timestamp by 1000 so we can remove three 000s from the timestamp.\n",
    "        # This makes the timestamp units seconds which matches the other data we have.\n",
    "        d.write(f'{ind},{int(int(line[0])/1000)},{fraction_price},{decimal_price}\\n')\n",
    "        # keep track of index\n",
    "        ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if csv are sorted\n",
    "for csv in list_of_csv:\n",
    "    data = pd.read_csv(bs.full_path(csv))\n",
    "    sorted_data = data.sort_values(by=['timestamp'])\n",
    "    print(f'{csv} is sorted?: {data.equals(sorted_data)}')\n",
    "    # The above resulted in \n",
    "    # full_data__6__2018.csv is sorted?: True\n",
    "    # full_data__6__2019.csv is sorted?: False\n",
    "    # full_data__6__2020.csv is sorted?: False\n",
    "    # full_data__6__2021.csv is sorted?: False\n",
    "    # So we have to sort the data (only do this once)\n",
    "    \n",
    "    # Test for nulls\n",
    "    null_counts = sorted_data[['timestamp', 'Open', 'Close', 'High', 'Low']].isnull().sum()\n",
    "    null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "    print('Null values found:')\n",
    "    print(null_counts)\n",
    "    # No nulls!\n",
    "\n",
    "    # Create 'price' column from avg of Open and Close\n",
    "    # First make the row and round to 4 decimals\n",
    "    # Divide by 4 after we make the fraction_price\n",
    "    sorted_data['decimal_price'] = round(sorted_data['Open'] + sorted_data['Close'] + sorted_data['High'] + sorted_data['Low'],\n",
    "         4)\n",
    "    # Then fractionalize the number to minimize floating point rounding errors\n",
    "    sorted_data['fraction_price'] = sorted_data['decimal_price'].apply(lambda x: frac(x)/4)\n",
    "    # Average out decimal price\n",
    "    sorted_data['decimal_price'] = sorted_data['decimal_price']/4\n",
    "    # Give the index a name for the csv\n",
    "    sorted_data.index.names = ['index']\n",
    "    # drop all columns we don't want\n",
    "    sorted_data = sorted_data.filter(['index', 'timestamp', 'fraction_price', 'decimal_price'])\n",
    "    # Only run this once\n",
    "    # NOTE: Uncomment this when running for the first time\n",
    "    sorted_data.to_csv(bs.full_path('sorted_full_data_'+ csv[-8:]))\n",
    "\n",
    "# Spacing\n",
    "print('\\nSorted check:')\n",
    "# Show that sorted data is sorted\n",
    "for csv in sorted_list_of_csv:\n",
    "    data = pd.read_csv(bs.full_path(csv))\n",
    "    sorted_data = data.sort_values(by=['timestamp'])\n",
    "    print(f'{csv} is sorted?: {data.equals(sorted_data)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Binance_ETH_all_price_data.csv\n",
      "Adding CoinBase_ETH_all_price_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Fraction' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VICTOR~1\\AppData\\Local\\Temp/ipykernel_29580/2347232850.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Adding {csv}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mcombined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombine_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Drop the fake index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Nextcloud Sync\\VsCode - Code\\Strat Performance testing\\Strategy_Performance\\init_data_helper.py\u001b[0m in \u001b[0;36mcombine_datasets\u001b[1;34m(df1, df2)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m# Set average decimal_price\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mcombined_dataframes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'decimal_price'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mcombined_dataframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_dataframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmake_average\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'decimal_price'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# Reset timestamp due to merge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8739\u001b[0m         )\n\u001b[1;32m-> 8740\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m                 \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m                     \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Nextcloud Sync\\VsCode - Code\\Strat Performance testing\\Strategy_Performance\\init_data_helper.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m# Set average decimal_price\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mcombined_dataframes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'decimal_price'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mcombined_dataframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_dataframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmake_average\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'decimal_price'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# Reset timestamp due to merge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Nextcloud Sync\\VsCode - Code\\Strat Performance testing\\Strategy_Performance\\init_data_helper.py\u001b[0m in \u001b[0;36mmake_average\u001b[1;34m(df_row, new_row_name)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# If we need decimals, round to the fourth digit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mnew_row_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'decimal_price'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mdf_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_row_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_row_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf_row\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_row_name\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;31m# Just default to column_1 for timestamps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mnew_row_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Fraction' and 'str'"
     ]
    }
   ],
   "source": [
    "# Combine ALL data into one big file\n",
    "\n",
    "# initialize list of all of the files we want to look at\n",
    "overall_list = [\n",
    "    'Binance_ETH_all_price_data.csv',\n",
    "    'CoinBase_ETH_all_price_data.csv'\n",
    "]\n",
    "# add the Kaggle data to our list\n",
    "overall_list = overall_list + sorted_list_of_csv\n",
    "\n",
    "# initialize empty dataframe to hold all the values\n",
    "combined_df = pd.DataFrame(columns=['index', 'timestamp', 'fraction_price', 'decimal_price'])\n",
    "\n",
    "# combine all the data into one dataset\n",
    "for csv in overall_list:\n",
    "    print(f'Adding {csv}')\n",
    "    new_data = pd.read_csv(bs.full_path(csv))\n",
    "    combined_df = idh.combine_datasets(combined_df, new_data)\n",
    "\n",
    "# Drop the fake index\n",
    "combined_df = combined_df.drop(columns=['index'])\n",
    "# Name the real index for the csv\n",
    "combined_df.index.names = ['index']\n",
    "\n",
    "# save result to a file\n",
    "combined_df.to_csv('csv_files/Combined_ETH_all_price_data.csv')\n",
    "print('Results Saved!\\n')\n",
    "\n",
    "# Check if we have any timestamp gaps in our data\n",
    "# Ideally we would only see the final timestamp printed\n",
    "idh.check_missing_timestamp(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idh.create_price_period('1/1/2018','1/5/2018', 'test')\n",
    "# idh.create_price_period('1/1/2018','2/1/2018', 'test_month')\n",
    "\n",
    "# --- Specific price_periods --- \n",
    "# Yearly\n",
    "idh.create_price_period('1/1/2018','1/1/2019', '2018_price_data')\n",
    "idh.create_price_period('1/1/2019','1/1/2020', '2019_price_data')\n",
    "idh.create_price_period('1/1/2020','1/1/2021', '2020_price_data')\n",
    "idh.create_price_period('1/1/2021','1/1/2022', '2021_price_data') \n",
    "# Past 4 Years - 2018 through 2021\n",
    "idh.create_price_period('1/1/2018', '1/1/2022', '2018-2021')\n",
    "# Past 3 Years - 2019 through 2021\n",
    "idh.create_price_period('1/1/2019', '1/1/2022', '2019-2021')\n",
    "# Past 2 Years - 2020 through 2021\n",
    "idh.create_price_period('1/1/2020', '1/1/2022', '2020-2021')\n",
    "# Past 1 Year - all of 2021 # Low to high to low to high\n",
    "idh.create_price_period('1/1/2021', '1/1/2022', '2021')\n",
    "\n",
    "# High to low \n",
    "# - 1515870180 (max of 2018) to end of 2018\n",
    "idh.create_price_period(1515870180, 1546300740, 'High-Low-1')\n",
    "# - 1620125000 (before 2021 crash) to 1627000000 (2021 crash low)\n",
    "idh.create_price_period(1620125000, 1627000000, 'High-Low-2')\n",
    "\n",
    "# Low to high \n",
    "# - start of 2020 to 1620125000 (before 2021 crash)\n",
    "idh.create_price_period('1/1/2020', 1620125000, 'Low-High-1')\n",
    "# - 1627000000 (2021 crash low) to end of 2021\n",
    "idh.create_price_period(1627000000, '1/1/2022', 'Low-High-2')\n",
    "\n",
    "# Low to high to low\n",
    "# - all of 2019\n",
    "idh.create_price_period('1/1/2019', 1577836740, 'Low-High-Low-1')\n",
    "# - 2021 start to 1627000000 (2021 crash low)\n",
    "idh.create_price_period('1/1/2021', 1627000000, 'Low-High-Low-2')\n",
    "\n",
    "# High to low to high\n",
    "# - 1515870180 (2018) to 1620125000 (before 2021 crash)\n",
    "idh.create_price_period(1515870180, 1620125000, 'High-Low-High-1')\n",
    "# - 1620125000 (before 2021 crash) to end of 2021\n",
    "idh.create_price_period(1620125000, '1/1/2022', 'High-Low-High-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalibrate time variable in a new row to start at 0\n",
    "# start_time = data['Time'].values[0]\n",
    "# data['Cal Time'] = data['Time'].apply(lambda x: x - start_time)\n",
    "\n",
    "# Examples\n",
    "# Plot data returned\n",
    "# By default x=index\n",
    "# data.plot(y='Price', kind='line')\n",
    "# data.plot(x='Cal Time', y=['Price', 'Plus .3%', 'Minus .3%'], kind='line')\n",
    "# data.plot(x='Cal Time', y=['Price', 'Plus .3%', 'Minus .3%'], kind='line', xlim=(35000,45000))\n",
    "# data.plot(x='Cal Time', y=['Price', 'Plus .3%', 'Minus .3%'], kind='line', ylim=(data['Price'].mean()*(1-.005),data['Price'].mean()*(1+.005)))\n",
    "\n",
    "# testing\n",
    "data = pd.read_csv(get_test_data_path('test.csv'), index_col='index')\n",
    "# print(type(data['fraction_price'].values[0]))\n",
    "# print(dir(data['price'].values[0]))\n",
    "start_datetime = pd.to_datetime(data['timestamp'].iloc[0], unit='s')\n",
    "end_datetime = pd.to_datetime(data['timestamp'].iloc[-1], unit='s')\n",
    "print(f'Start datetime: {start_datetime}')\n",
    "print(f'End datetime:   {end_datetime}')\n",
    "data.plot(x='timestamp', y='decimal_price', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly Price Plots\n",
    "for csv in sorted_list_of_csv:\n",
    "    data = pd.read_csv(bs.full_path(csv), index_col='index')\n",
    "    start_datetime = pd.to_datetime(data['timestamp'].iloc[0], unit='s')\n",
    "    end_datetime = pd.to_datetime(data['timestamp'].iloc[-1], unit='s')\n",
    "    print(f'Start datetime: {start_datetime}')\n",
    "    print(f'End datetime:   {end_datetime}')\n",
    "    data.plot(x='timestamp', y='decimal_price', title=csv, kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = 'Combined_ETH_all_price_data.csv'\n",
    "print(csv)\n",
    "data = pd.read_csv(bs.full_path(csv), index_col='index')\n",
    "start_datetime = pd.to_datetime(data['timestamp'].iloc[0], unit='s')\n",
    "end_datetime = pd.to_datetime(data['timestamp'].iloc[-1], unit='s')\n",
    "print(f'Start datetime: {start_datetime}')\n",
    "print(f'End datetime:   {end_datetime}')\n",
    "data.plot(x='timestamp', y='decimal_price', title=csv, kind='line')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7f6ce40941103abb3416a795e028450c6c1ee6bd4c25de01325501f9db33dc9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
