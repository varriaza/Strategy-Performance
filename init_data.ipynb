{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used for creating price data, analysing that data and creating price periods.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import lib.base_strategy as bs\n",
    "import lib.init_data_helper as idh\n",
    "from fractions import Fraction as frac\n",
    "from tests.test_all_tests import get_test_data_path\n",
    "\n",
    "from lib.get_binance_data import get_binance_data\n",
    "from lib.get_coinbase_data import get_coinbase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv's\n",
    "\n",
    "# Collect these files from Kaggle here:\n",
    "# https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-ethereum\n",
    "list_of_csv = [\n",
    "    # TODO: figure out problems with 2017 data\n",
    "    # 'full_data__6__2017.csv', # don't use 2017 data, its not great\n",
    "    'full_data__6__2018.csv',\n",
    "    'full_data__6__2019.csv',\n",
    "    'full_data__6__2020.csv',\n",
    "    'full_data__6__2021.csv'\n",
    "]\n",
    "sorted_list_of_csv = [\n",
    "    # 'sorted_full_data_2017.csv',\n",
    "    'sorted_full_data_2018.csv',\n",
    "    'sorted_full_data_2019.csv',\n",
    "    'sorted_full_data_2020.csv',\n",
    "    'sorted_full_data_2021.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CoinBase Data - Takes around ~330 min to collect all historic data\n",
    "\n",
    "# Make sure we actually want to get the data\n",
    "get_data = True\n",
    "if get_data:\n",
    "    # Split data collection into two parts so that it doesn't error out\n",
    "    part_1 = get_coinbase_data(start_date='2018-01-01-00-00', end_date='2020-01-1-00-00')\n",
    "    # Don't pass end_date to use current time as end\n",
    "    part_2 = get_coinbase_data(start_date='2020-01-01-00-00')\n",
    "    print('Querying complete.')\n",
    "\n",
    "    # Combine parts\n",
    "    coinbase_data = idh.combine_datasets(part_1, part_2)\n",
    "    # drop index so we don't get two index columns\n",
    "    coinbase_data = coinbase_data.drop(columns=['index'])\n",
    "    print('Combining data complete.')\n",
    "    # Give index a name\n",
    "    coinbase_data.index.names = ['index']\n",
    "\n",
    "    # Save data\n",
    "    print(f'\\nHistorical values found: {len(coinbase_data.index)}')\n",
    "    coinbase_data.to_csv(bs.full_path('CoinBase_ETH_all_price_data'))\n",
    "    print(f'\\nSubset of final data:\\n{coinbase_data.tail(5)}')\n",
    "    print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing timestamps: 14751\n",
      "Missing timestamps: \n",
      "[1516145640 1516147800 1516308000 ... 1637918340 1637921460 1642849500]\n",
      "\n",
      "\n",
      "Looking for a difference of 10.0% or more.\n",
      "Number of jumps: 0\n",
      "Big Jumps: \n",
      "Empty DataFrame\n",
      "Columns: [index, timestamp, fraction_price, decimal_price, next_decimal_price, multiplier]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# CoinBase price data analysis\n",
    "# Read the dataframe in case we are just running this cell on its own\n",
    "coinbase_df = pd.read_csv(bs.full_path('CoinBase_ETH_all_price_data'))\n",
    "# Check if we have any timestamp gaps in our data.\n",
    "idh.check_missing_timestamp(coinbase_df)\n",
    "print('\\n') # Spacing\n",
    "\n",
    "# Test price jumps, up or down by more than a specific amount.\n",
    "# This can help show where our price data may be bad or incomplete.\n",
    "idh.check_price_jump(coinbase_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Binance Data - Takes around ~80 min to collect all historic data\n",
    "\n",
    "# Make sure we actually want to get the data\n",
    "get_data = True\n",
    "if get_data:\n",
    "    # USDT has more historical price data than USDC but either can be used\n",
    "    TRADING_PAIR = 'ETHUSDT'\n",
    "    # TRADING_PAIR = 'ETHUSDC'\n",
    "    if TRADING_PAIR == 'ETHUSDT':\n",
    "        ## For ETH USDT\n",
    "        # Earliest timestamp found: 1502942400.0\n",
    "        # Human readable format: Wed Aug 16 21:00:00 2017\n",
    "        # However, early Binance ETH USDT data is not very good so only use 2018+ (aka 1514764800+)\n",
    "        # Multiply by 1000 to make what we would get from the query\n",
    "        start_date = 1514764800*1000\n",
    "        # For ETH USDC (not quite as much data as USDT)\n",
    "        # Earliest timestamp found: 1544844060.0\n",
    "        # Human readable format: Fri Dec 14 19:21:00 2018\n",
    "    else:\n",
    "        start_date = ''\n",
    "\n",
    "    df_klines = get_binance_data(TRADING_PAIR, start_date)\n",
    "\n",
    "    # save data as a file\n",
    "    save_file_name = 'Binance_ETH_all_price_data.csv'\n",
    "    df_klines.to_csv(bs.full_path(save_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing timestamps: 30\n",
      "Missing timestamps: \n",
      "[1515034860 1518049740 1518242414 1518321660 1529978400 1530104400\n",
      " 1530663780 1539928800 1542160800 1552356000 1557889200 1559942040\n",
      " 1565834400 1573610400 1573623000 1574647200 1581213600 1582112160\n",
      " 1583313720 1587780000 1593309600 1606716000 1608559740 1608861600\n",
      " 1613014860 1614996000 1618884000 1619323260 1628820000 1632898800]\n",
      "\n",
      "\n",
      "Looking for a difference of 10.0% or more.\n",
      "Number of jumps: 0\n",
      "Big Jumps: \n",
      "Empty DataFrame\n",
      "Columns: [index, timestamp, fraction_price, decimal_price, next_decimal_price, multiplier]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Binance price data analysis\n",
    "# Read the dataframe in case we are just running this cell on its own\n",
    "binance_df = pd.read_csv(bs.full_path('Binance_ETH_all_price_data.csv'))\n",
    "# Check if we have any timestamp gaps in our data.\n",
    "idh.check_missing_timestamp(binance_df)\n",
    "print('\\n') # Spacing\n",
    "\n",
    "# Test price jumps, up or down by more than a specific amount.\n",
    "# This can help show where our price data may be bad or incomplete.\n",
    "idh.check_price_jump(binance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if csv are sorted\n",
    "for csv in list_of_csv:\n",
    "    data = pd.read_csv(bs.full_path(csv))\n",
    "    sorted_data = data.sort_values(by=['timestamp'])\n",
    "    print(f'{csv} is sorted?: {data.equals(sorted_data)}')\n",
    "    # The above resulted in \n",
    "    # full_data__6__2018.csv is sorted?: True\n",
    "    # full_data__6__2019.csv is sorted?: False\n",
    "    # full_data__6__2020.csv is sorted?: False\n",
    "    # full_data__6__2021.csv is sorted?: False\n",
    "    # So we have to sort the data (only do this once)\n",
    "    \n",
    "    # Test for nulls\n",
    "    null_counts = sorted_data[['timestamp', 'Open', 'Close', 'High', 'Low']].isnull().sum()\n",
    "    null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "    print('Null values found:')\n",
    "    print(null_counts)\n",
    "    # No nulls!\n",
    "\n",
    "    # Create 'price' column from avg of Open and Close\n",
    "    # First make the row and round to 4 decimals\n",
    "    # Divide by 4 after we make the fraction_price\n",
    "    sorted_data['decimal_price'] = round(sorted_data['Open'] + sorted_data['Close'] + sorted_data['High'] + sorted_data['Low'],\n",
    "         4)\n",
    "    # Then fractionalize the number to minimize floating point rounding errors\n",
    "    sorted_data['fraction_price'] = sorted_data['decimal_price'].apply(lambda x: frac(x)/4)\n",
    "    # Average out decimal price\n",
    "    sorted_data['decimal_price'] = sorted_data['decimal_price']/4\n",
    "    # Give the index a name for the csv\n",
    "    sorted_data.index.names = ['index']\n",
    "    # drop all columns we don't want\n",
    "    sorted_data = sorted_data.filter(['index', 'timestamp', 'fraction_price', 'decimal_price'])\n",
    "    # Only run this once\n",
    "    # NOTE: Uncomment this when running for the first time\n",
    "    # sorted_data.to_csv(bs.full_path('sorted_full_data_'+ csv[-8:]))\n",
    "\n",
    "# Spacing\n",
    "print('\\nSorted check:')\n",
    "# Show that sorted data is sorted\n",
    "for csv in sorted_list_of_csv:\n",
    "    data = pd.read_csv(bs.full_path(csv))\n",
    "    sorted_data = data.sort_values(by=['timestamp'])\n",
    "    print(f'{csv} is sorted?: {data.equals(sorted_data)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ALL data into one big file\n",
    "\n",
    "# initialize list of all of the files we want to look at\n",
    "overall_list = [\n",
    "    'Binance_ETH_all_price_data.csv',\n",
    "    'CoinBase_ETH_all_price_data.csv'\n",
    "]\n",
    "# add the Kaggle data to our list\n",
    "overall_list = overall_list + sorted_list_of_csv\n",
    "\n",
    "# initialize empty dataframe to hold all the values\n",
    "combined_df = pd.DataFrame(columns=['index', 'timestamp', 'fraction_price', 'decimal_price'])\n",
    "\n",
    "# combine all the data into one dataset\n",
    "for csv in overall_list:\n",
    "    print(f'Adding {csv}')\n",
    "    new_data = pd.read_csv(bs.full_path(csv))\n",
    "    combined_df = idh.combine_datasets(combined_df, new_data)\n",
    "\n",
    "# Drop the fake index\n",
    "combined_df = combined_df.drop(columns=['index'])\n",
    "# Name the real index for the csv\n",
    "combined_df.index.names = ['index']\n",
    "\n",
    "# save result to a file\n",
    "combined_df.to_csv(bs.full_path('Combined_ETH_all_price_data.csv'))\n",
    "print('Results Saved!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing timestamps: 1\n",
      "Missing timestamps: \n",
      "[1518242414]\n",
      "\n",
      "\n",
      "Looking for a difference of 10.0% or more.\n",
      "Number of jumps: 2\n",
      "Big Jumps: \n",
      "    index   timestamp                     fraction_price  decimal_price  next_decimal_price  multiplier\n",
      "0  505646  1545031500  24004150829512655/281474976710656        85.2799            101.1726        1.19\n",
      "1  505648  1545031620      906617046138513/8796093022208       103.0704             85.3522        0.83\n"
     ]
    }
   ],
   "source": [
    "# Price data analysis\n",
    "# Validate our final combined dataframe\n",
    "combined_df = pd.read_csv(bs.full_path('Combined_ETH_all_price_data.csv'))\n",
    "\n",
    "# Check if we have any timestamp gaps in our data.\n",
    "# This could affect strategy performance if enough are present.\n",
    "# Adding more data sources will reduce breaks in timestamp data and will make our average price more accurate due to \n",
    "    # being less sensitive to the price on one exchange.\n",
    "idh.check_missing_timestamp(combined_df)\n",
    "print('\\n') # Spacing\n",
    "\n",
    "# Test price jumps, up or down by more than a specific amount.\n",
    "# This can help show where our price data may be bad or incomplete.\n",
    "idh.check_price_jump(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idh.create_price_period('1/1/2018','1/5/2018', 'test')\n",
    "# idh.create_price_period('1/1/2018','2/1/2018', 'test_month')\n",
    "\n",
    "# --- Specific price_periods --- \n",
    "# Yearly\n",
    "idh.create_price_period('1/1/2018','1/1/2019', '2018_price_data')\n",
    "idh.create_price_period('1/1/2019','1/1/2020', '2019_price_data')\n",
    "idh.create_price_period('1/1/2020','1/1/2021', '2020_price_data')\n",
    "idh.create_price_period('1/1/2021','1/1/2022', '2021_price_data') \n",
    "# Past 4 Years - 2018 through 2021\n",
    "idh.create_price_period('1/1/2018', '1/1/2022', '2018-2021')\n",
    "# Past 3 Years - 2019 through 2021\n",
    "idh.create_price_period('1/1/2019', '1/1/2022', '2019-2021')\n",
    "# Past 2 Years - 2020 through 2021\n",
    "idh.create_price_period('1/1/2020', '1/1/2022', '2020-2021')\n",
    "# Past 1 Year - all of 2021 # Low to high to low to high\n",
    "idh.create_price_period('1/1/2021', '1/1/2022', '2021')\n",
    "\n",
    "# High to low \n",
    "# - 1515870180 (max of 2018) to end of 2018\n",
    "idh.create_price_period(1515870180, 1546300740, 'High-Low-1')\n",
    "# - 1620125000 (before 2021 crash) to 1627000000 (2021 crash low)\n",
    "idh.create_price_period(1620125000, 1627000000, 'High-Low-2')\n",
    "\n",
    "# Low to high \n",
    "# - start of 2020 to 1620125000 (before 2021 crash)\n",
    "idh.create_price_period('1/1/2020', 1620125000, 'Low-High-1')\n",
    "# - 1627000000 (2021 crash low) to end of 2021\n",
    "idh.create_price_period(1627000000, '1/1/2022', 'Low-High-2')\n",
    "\n",
    "# Low to high to low\n",
    "# - all of 2019\n",
    "idh.create_price_period('1/1/2019', 1577836740, 'Low-High-Low-1')\n",
    "# - 2021 start to 1627000000 (2021 crash low)\n",
    "idh.create_price_period('1/1/2021', 1627000000, 'Low-High-Low-2')\n",
    "\n",
    "# High to low to high\n",
    "# - 1515870180 (2018) to 1620125000 (before 2021 crash)\n",
    "idh.create_price_period(1515870180, 1620125000, 'High-Low-High-1')\n",
    "# - 1620125000 (before 2021 crash) to end of 2021\n",
    "idh.create_price_period(1620125000, '1/1/2022', 'High-Low-High-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalibrate time variable in a new row to start at 0\n",
    "# start_time = data['Time'].values[0]\n",
    "# data['Cal Time'] = data['Time'].apply(lambda x: x - start_time)\n",
    "\n",
    "# Examples\n",
    "# Plot data returned\n",
    "# By default x=index\n",
    "# data.plot(y='Price', kind='line')\n",
    "# data.plot(x='Cal Time', y=['Price', 'Plus .3%', 'Minus .3%'], kind='line')\n",
    "# data.plot(x='Cal Time', y=['Price', 'Plus .3%', 'Minus .3%'], kind='line', xlim=(35000,45000))\n",
    "# data.plot(x='Cal Time', y=['Price', 'Plus .3%', 'Minus .3%'], kind='line', ylim=(data['Price'].mean()*(1-.005),data['Price'].mean()*(1+.005)))\n",
    "\n",
    "# testing\n",
    "data = pd.read_csv(get_test_data_path('test.csv'), index_col='index')\n",
    "# print(type(data['fraction_price'].values[0]))\n",
    "# print(dir(data['price'].values[0]))\n",
    "start_datetime = pd.to_datetime(data['timestamp'].iloc[0], unit='s')\n",
    "end_datetime = pd.to_datetime(data['timestamp'].iloc[-1], unit='s')\n",
    "print(f'Start datetime: {start_datetime}')\n",
    "print(f'End datetime:   {end_datetime}')\n",
    "data.plot(x='timestamp', y='decimal_price', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly Price Plots\n",
    "for csv in sorted_list_of_csv:\n",
    "    data = pd.read_csv(bs.full_path(csv), index_col='index')\n",
    "    start_datetime = pd.to_datetime(data['timestamp'].iloc[0], unit='s')\n",
    "    end_datetime = pd.to_datetime(data['timestamp'].iloc[-1], unit='s')\n",
    "    print(f'Start datetime: {start_datetime}')\n",
    "    print(f'End datetime:   {end_datetime}')\n",
    "    data.plot(x='timestamp', y='decimal_price', title=csv, kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = 'Combined_ETH_all_price_data.csv'\n",
    "print(csv)\n",
    "data = pd.read_csv(bs.full_path(csv), index_col='index')\n",
    "start_datetime = pd.to_datetime(data['timestamp'].iloc[0], unit='s')\n",
    "end_datetime = pd.to_datetime(data['timestamp'].iloc[-1], unit='s')\n",
    "print(f'Start datetime: {start_datetime}')\n",
    "print(f'End datetime:   {end_datetime}')\n",
    "data.plot(x='timestamp', y='decimal_price', title=csv, kind='line')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7f6ce40941103abb3416a795e028450c6c1ee6bd4c25de01325501f9db33dc9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
