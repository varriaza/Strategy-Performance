{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Used for ploting csv data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import pandas as pd\n",
    "import lib.base_strategy as bs\n",
    "import lib.init_data_helper as idh\n",
    "from fractions import Fraction as frac\n",
    "from tests.test_all_tests import get_test_data_path\n",
    "\n",
    "from lib.get_binance_data import get_binance_data\n",
    "from lib.get_coinbase_data import get_coinbase_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv's\n",
    "\n",
    "# Collect these files from Kaggle here:\n",
    "# https://www.kaggle.com/yamqwe/cryptocurrency-extra-data-ethereum\n",
    "list_of_csv = [\n",
    "    # TODO: figure out problems with 2017 data\n",
    "    # 'full_data__6__2017.csv', # don't use 2017 data, its not great\n",
    "    'full_data__6__2018.csv',\n",
    "    'full_data__6__2019.csv',\n",
    "    'full_data__6__2020.csv',\n",
    "    'full_data__6__2021.csv'\n",
    "]\n",
    "sorted_list_of_csv = [\n",
    "    # 'sorted_full_data_2017.csv',\n",
    "    'sorted_full_data_2018.csv',\n",
    "    'sorted_full_data_2019.csv',\n",
    "    'sorted_full_data_2020.csv',\n",
    "    'sorted_full_data_2021.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subset of data:\n",
      "   index   timestamp                  fraction_price  decimal_price\n",
      "0      0  1514764860                          1479/2       739.5000\n",
      "1      1  1514764920  1625809361085399/2199023255552       739.3325\n",
      "2      2  1514764980   812789231821783/1099511627776       739.2275\n",
      "3      3  1514765040  6505480448062259/8796093022208       739.5875\n",
      "4      4  1514765100  3252025541473075/4398046511104       739.4250\n",
      "\n",
      "Subset of data:\n",
      "   index   timestamp                  fraction_price  decimal_price\n",
      "0      0  1514782860  3289320975887237/4398046511104        747.905\n",
      "1      1  1514782920  6568086640147825/8796093022208        746.705\n",
      "2      2  1514782980  6564788105264497/8796093022208        746.330\n",
      "3      3  1514783040  6564040437357609/8796093022208        746.245\n",
      "4      4  1514783100  6565051988055163/8796093022208        746.360\n",
      "Querying complete.\n",
      "Combining data complete.\n",
      "\n",
      "Historical values found: 300\n",
      "\n",
      "Subset of final data:\n",
      "        timestamp                    fraction_price  decimal_price\n",
      "index                                                             \n",
      "295    1514782560    6588042776191959/8796093022208       748.9737\n",
      "296    1514782620  13173182841686589/17592186044416       748.8087\n",
      "297    1514782680  13171643525407703/17592186044416       748.7213\n",
      "298    1514782740     823303311762391/1099511627776       748.7900\n",
      "299    1514782800  13171819447268147/17592186044416       748.7313\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get CoinBase Data\n",
    "\n",
    "# Make sure we actually want to get the data\n",
    "get_data = True\n",
    "\n",
    "if get_data:\n",
    "    # Split data collection into two parts so that it doesn't error out\n",
    "    part_1 = get_coinbase_data(start_date='2018-01-01-00-00', end_date='2020-01-1-00-00')\n",
    "    # Don't pass end_date to use current time as end\n",
    "    part_2 = get_coinbase_data(start_date='2020-01-01-00-00')\n",
    "    print('Querying complete.')\n",
    "\n",
    "    # Combine parts\n",
    "    coinbase_data = idh.combine_datasets(part_1, part_2)\n",
    "    # drop index so we don't get two index columns\n",
    "    coinbase_data = coinbase_data.drop(columns=['index'])\n",
    "    print('Combining data complete.')\n",
    "    # Give index a name\n",
    "    coinbase_data.index.names = ['index']\n",
    "    \n",
    "    # Save data\n",
    "    print(f'\\nHistorical values found: {len(coinbase_data.index)}')\n",
    "    coinbase_data.to_csv(bs.full_path('CoinBase_ETH_all_price_data'))\n",
    "    print(f'\\nSubset of final data:\\n{coinbase_data.tail(5)}')\n",
    "    print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Binance Data\n",
    "\n",
    "# Make sure we actually want to get the data\n",
    "get_data = False\n",
    "\n",
    "if get_data:\n",
    "    # USDT has more historical price data than USDC but either can be used\n",
    "    TRADING_PAIR = 'ETHUSDT'\n",
    "    # TRADING_PAIR = 'ETHUSDC'\n",
    "    if TRADING_PAIR == 'ETHUSDT':\n",
    "        ## For ETH USDT\n",
    "        # Earliest timestamp found: 1502942400.0\n",
    "        # Human readable format: Wed Aug 16 21:00:00 2017\n",
    "        # However, early Binance ETH USDT data is not very good so only use 2018+ (aka 1514764800+)\n",
    "        # Multiply by 1000 to make what we would get from the query\n",
    "        start_date = 1514764800*1000\n",
    "        # For ETH USDC (not quite as much data as USDT)\n",
    "        # Earliest timestamp found: 1544844060.0\n",
    "        # Human readable format: Fri Dec 14 19:21:00 2018\n",
    "    else:\n",
    "        start_date = ''\n",
    "\n",
    "    klines = get_binance_data(TRADING_PAIR, start_date)\n",
    "\n",
    "    # save data as a file\n",
    "    save_file_name = 'csv_files/Binance_ETH_all_price_data.csv'\n",
    "    with open(save_file_name, 'w') as d:\n",
    "    # with open('csv_files/ETH_all_price_data.csv', 'w') as d:\n",
    "        d.write('index,timestamp,fraction_price,decimal_price\\n')\n",
    "        # add index column\n",
    "        ind = 0\n",
    "        for line in klines:\n",
    "            fraction_price = frac(float(line[1])+float(line[2])+float(line[3])+float(line[4]))/4\n",
    "            decimal_price = round(fraction_price, 4)\n",
    "            # Divide timestamp by 1000 so we can remove three 000s from the timestamp.\n",
    "            # This makes the timestamp units seconds which matches the other data we have.\n",
    "            d.write(f'{ind},{int(int(line[0])/1000)},{fraction_price},{decimal_price}\\n')\n",
    "            # keep track of index\n",
    "            ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if csv are sorted\n",
    "for csv in list_of_csv:\n",
    "    data = pd.read_csv(bs.full_path(csv))\n",
    "    sorted_data = data.sort_values(by=['timestamp'])\n",
    "    print(f'{csv} is sorted?: {data.equals(sorted_data)}')\n",
    "    # The above resulted in \n",
    "    # full_data__6__2018.csv is sorted?: True\n",
    "    # full_data__6__2019.csv is sorted?: False\n",
    "    # full_data__6__2020.csv is sorted?: False\n",
    "    # full_data__6__2021.csv is sorted?: False\n",
    "    # So we have to sort the data (only do this once)\n",
    "    \n",
    "    # Test for nulls\n",
    "    null_counts = sorted_data[['timestamp', 'Open', 'Close', 'High', 'Low']].isnull().sum()\n",
    "    null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "    print('Null values found:')\n",
    "    print(null_counts)\n",
    "    # No nulls!\n",
    "\n",
    "    # Create 'price' column from avg of Open and Close\n",
    "    # First make the row and round to 4 decimals\n",
    "    # Divide by 4 after we make the fraction_price\n",
    "    sorted_data['decimal_price'] = round(sorted_data['Open'] + sorted_data['Close'] + sorted_data['High'] + sorted_data['Low'],\n",
    "         4)\n",
    "    # Then fractionalize the number to minimize floating point rounding errors\n",
    "    sorted_data['fraction_price'] = sorted_data['decimal_price'].apply(lambda x: frac(x)/4)\n",
    "    # Average out decimal price\n",
    "    sorted_data['decimal_price'] = sorted_data['decimal_price']/4\n",
    "    # Give the index a name for the csv\n",
    "    sorted_data.index.names = ['index']\n",
    "    # drop all columns we don't want\n",
    "    sorted_data = sorted_data.filter(['index', 'timestamp', 'fraction_price', 'decimal_price'])\n",
    "    # Only run this once\n",
    "    # NOTE: Uncomment this when running for the first time\n",
    "    sorted_data.to_csv(bs.full_path('sorted_full_data_'+ csv[-8:]))\n",
    "\n",
    "# Spacing\n",
    "print('\\nSorted check:')\n",
    "# Show that sorted data is sorted\n",
    "for csv in sorted_list_of_csv:\n",
    "    data = pd.read_csv(bs.full_path(csv))\n",
    "    sorted_data = data.sort_values(by=['timestamp'])\n",
    "    print(f'{csv} is sorted?: {data.equals(sorted_data)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ALL data into one big file\n",
    "\n",
    "# initialize list of all of the files we want to look at\n",
    "overall_list = [\n",
    "    'Binance_ETH_all_price_data.csv',\n",
    "    'CoinBase_ETH_all_price_data.csv'\n",
    "]\n",
    "# add the Kaggle data to our list\n",
    "overall_list = overall_list + sorted_list_of_csv\n",
    "\n",
    "# initialize empty dataframe to hold all the values\n",
    "combined_df = pd.DataFrame(columns=['index', 'timestamp', 'fraction_price', 'decimal_price'])\n",
    "\n",
    "# combine all the data into one dataset\n",
    "for csv in overall_list:\n",
    "    print(f'Adding {csv}')\n",
    "    new_data = pd.read_csv(bs.full_path(csv))\n",
    "    combined_df = idh.combine_datasets(combined_df, new_data)\n",
    "\n",
    "# Drop the fake index\n",
    "combined_df = combined_df.drop(columns=['index'])\n",
    "# Name the real index for the csv\n",
    "combined_df.index.names = ['index']\n",
    "\n",
    "# save result to a file\n",
    "combined_df.to_csv('csv_files/Combined_ETH_all_price_data.csv')\n",
    "print('Results Saved!\\n')\n",
    "\n",
    "# Check if we have any timestamp gaps in our data\n",
    "# Ideally we would only see the final timestamp printed\n",
    "idh.check_missing_timestamp(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idh.create_price_period('1/1/2018','1/5/2018', 'test')\n",
    "# idh.create_price_period('1/1/2018','2/1/2018', 'test_month')\n",
    "\n",
    "# --- Specific price_periods --- \n",
    "# Yearly\n",
    "idh.create_price_period('1/1/2018','1/1/2019', '2018_price_data')\n",
    "idh.create_price_period('1/1/2019','1/1/2020', '2019_price_data')\n",
    "idh.create_price_period('1/1/2020','1/1/2021', '2020_price_data')\n",
    "idh.create_price_period('1/1/2021','1/1/2022', '2021_price_data') \n",
    "# Past 4 Years - 2018 through 2021\n",
    "idh.create_price_period('1/1/2018', '1/1/2022', '2018-2021')\n",
    "# Past 3 Years - 2019 through 2021\n",
    "idh.create_price_period('1/1/2019', '1/1/2022', '2019-2021')\n",
    "# Past 2 Years - 2020 through 2021\n",
    "idh.create_price_period('1/1/2020', '1/1/2022', '2020-2021')\n",
    "# Past 1 Year - all of 2021 # Low to high to low to high\n",
    "idh.create_price_period('1/1/2021', '1/1/2022', '2021')\n",
    "\n",
    "# High to low \n",
    "# - 1515870180 (max of 2018) to end of 2018\n",
    "idh.create_price_period(1515870180, 1546300740, 'High-Low-1')\n",
    "# - 1620125000 (before 2021 crash) to 1627000000 (2021 crash low)\n",
    "idh.create_price_period(1620125000, 1627000000, 'High-Low-2')\n",
    "\n",
    "# Low to high \n",
    "# - start of 2020 to 1620125000 (before 2021 crash)\n",
    "idh.create_price_period('1/1/2020', 1620125000, 'Low-High-1')\n",
    "# - 1627000000 (2021 crash low) to end of 2021\n",
    "idh.create_price_period(1627000000, '1/1/2022', 'Low-High-2')\n",
    "\n",
    "# Low to high to low\n",
    "# - all of 2019\n",
    "idh.create_price_period('1/1/2019', 1577836740, 'Low-High-Low-1')\n",
    "# - 2021 start to 1627000000 (2021 crash low)\n",
    "idh.create_price_period('1/1/2021', 1627000000, 'Low-High-Low-2')\n",
    "\n",
    "# High to low to high\n",
    "# - 1515870180 (2018) to 1620125000 (before 2021 crash)\n",
    "idh.create_price_period(1515870180, 1620125000, 'High-Low-High-1')\n",
    "# - 1620125000 (before 2021 crash) to end of 2021\n",
    "idh.create_price_period(1620125000, '1/1/2022', 'High-Low-High-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalibrate time variable in a new row to start at 0\n",
    "# start_time = data['Time'].values[0]\n",
    "# data['Cal Time'] = data['Time'].apply(lambda x: x - start_time)\n",
    "\n",
    "# Examples\n",
    "# Plot data returned\n",
    "# By default x=index\n",
    "# data.plot(y='Price', kind='line')\n",
    "# data.plot(x='Cal Time', y=['Price', 'Plus .3%', 'Minus .3%'], kind='line')\n",
    "# data.plot(x='Cal Time', y=['Price', 'Plus .3%', 'Minus .3%'], kind='line', xlim=(35000,45000))\n",
    "# data.plot(x='Cal Time', y=['Price', 'Plus .3%', 'Minus .3%'], kind='line', ylim=(data['Price'].mean()*(1-.005),data['Price'].mean()*(1+.005)))\n",
    "\n",
    "# testing\n",
    "data = pd.read_csv(get_test_data_path('test.csv'), index_col='index')\n",
    "# print(type(data['fraction_price'].values[0]))\n",
    "# print(dir(data['price'].values[0]))\n",
    "start_datetime = pd.to_datetime(data['timestamp'].iloc[0], unit='s')\n",
    "end_datetime = pd.to_datetime(data['timestamp'].iloc[-1], unit='s')\n",
    "print(f'Start datetime: {start_datetime}')\n",
    "print(f'End datetime:   {end_datetime}')\n",
    "data.plot(x='timestamp', y='decimal_price', kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yearly Price Plots\n",
    "for csv in sorted_list_of_csv:\n",
    "    data = pd.read_csv(bs.full_path(csv), index_col='index')\n",
    "    start_datetime = pd.to_datetime(data['timestamp'].iloc[0], unit='s')\n",
    "    end_datetime = pd.to_datetime(data['timestamp'].iloc[-1], unit='s')\n",
    "    print(f'Start datetime: {start_datetime}')\n",
    "    print(f'End datetime:   {end_datetime}')\n",
    "    data.plot(x='timestamp', y='decimal_price', title=csv, kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = 'Combined_ETH_all_price_data.csv'\n",
    "print(csv)\n",
    "data = pd.read_csv(bs.full_path(csv), index_col='index')\n",
    "start_datetime = pd.to_datetime(data['timestamp'].iloc[0], unit='s')\n",
    "end_datetime = pd.to_datetime(data['timestamp'].iloc[-1], unit='s')\n",
    "print(f'Start datetime: {start_datetime}')\n",
    "print(f'End datetime:   {end_datetime}')\n",
    "data.plot(x='timestamp', y='decimal_price', title=csv, kind='line')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7f6ce40941103abb3416a795e028450c6c1ee6bd4c25de01325501f9db33dc9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
